{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate the importance of distance/similarity metric over the Reuters\n",
    "Text Categorization Collection. The database is available at [kdd.ics.uci.edu](https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html)\n",
    "\n",
    "Using a KNeighborsClassifier(n_neighbors=k, metric=metric,\n",
    "weights=’distance’), with different values for the number of neigh-\n",
    "bors k and at least 3 different metrics. (see Lab3 for example).\n",
    "\n",
    "Compare the following text vectorization strategies and discuss the\n",
    "results when using:\n",
    "* raw word frequencies\n",
    "* Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "* encoding/embedding method of your choice (e.g., BOW, Word2Vec,\n",
    "GloVe)\n",
    "\n",
    "Use the above obtained high-dimensional data set (with one of the\n",
    "representations - raw word frequencies, TF-IDF, etc.) and apply di-\n",
    "mensionality reduction techniques (e.g., PCA, t-SNE, UMAP). Again,\n",
    "analyze the performance of a classification model of your choice for the\n",
    "initial and for the reduced data produced using two different dimen-\n",
    "sionality reduction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data set loading and summary\n",
    " In this section, we load the dataset and also print a small summary of the dataset\n",
    " We also import all the things we might need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/acasa/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/acasa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/acasa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/acasa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/acasa/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.data.path.append('C:\\\\Users\\\\Magda\\\\AppData\\\\Roaming\\\\nltk_data')\n",
    "# The NLTK library alread containes the Reuters-21578 data set\n",
    "#  and all we have to do is import it from NLTK\n",
    "from nltk.corpus import reuters\n",
    "# import nltk.data\n",
    "\n",
    "# We also import a dataset of 'stop words' which are general words\n",
    "#  such as 'the', 'of', 'and' and are usually meaningless.\n",
    "# We use this stopwords list to remove them from the Reuters data set.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# To use the Reuters-21578 from within NLTK, we have to download it first.\n",
    "nltk.download(\"reuters\") # run only once, after that it will be ignored anyway\n",
    "\n",
    "# To use the stopwords from within NLTK, we have to download them first.\n",
    "nltk.download(\"stopwords\") # run only once, after that it will be ignored anyway\n",
    "\n",
    "# WordNet corpus required by the WordNetLemmatizer\n",
    "nltk.download('wordnet') # run only once, after that it will be ignored anyway\n",
    "\n",
    "# Data for NLTK word tokenizer, otherwise word_tokenize() will not work\n",
    "nltk.download('punkt') # run only once, after that it will be ignored anyway\n",
    "nltk.download('punkt_tab') # run only once, after that it will be ignored anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters-21578 number of tags: 10788\n",
      "Reuters-21578 number of documents: 10788\n"
     ]
    }
   ],
   "source": [
    "# Print a quick summary of the Reuters-21578 dataset\n",
    "fileIds = reuters.fileids()\n",
    "print(\"Reuters-21578 number of tags:\", len(fileIds))\n",
    "print(\"Reuters-21578 number of documents:\", len(fileIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters-21578 number of train documents: 7769\n",
      "Reuters-21578 number of test documents: 3019\n",
      "Reuters-21578 number of categories: 90\n"
     ]
    }
   ],
   "source": [
    "# Find all the train documents and print their number\n",
    "trainDocuments = list(filter(lambda doc: doc.startswith(\"train\"), fileIds))\n",
    "print(\"Reuters-21578 number of train documents:\", len(trainDocuments))\n",
    "\n",
    "# Find all the test documents and print their number\n",
    "testDocuments = list(filter(lambda doc: doc.startswith(\"test\"), fileIds))\n",
    "print(\"Reuters-21578 number of test documents:\", len(testDocuments))\n",
    "\n",
    "# Print the number of categories from the Reuters-21578 dataset\n",
    "categories = reuters.categories()\n",
    "print(\"Reuters-21578 number of categories:\", len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters-21578 categories:\n",
      "acq          cpi            hog          meal-feed     platinum  soy-oil        \n",
      "alum         cpu            housing      money-fx      potato    soybean        \n",
      "barley       crude          income       money-supply  propane   strategic-metal\n",
      "bop          dfl            instal-debt  naphtha       rand      sugar          \n",
      "carcass      dlr            interest     nat-gas       rape-oil  sun-meal       \n",
      "castor-oil   dmk            ipi          nickel        rapeseed  sun-oil        \n",
      "cocoa        earn           iron-steel   nkr           reserves  sunseed        \n",
      "coconut      fuel           jet          nzdlr         retail    tea            \n",
      "coconut-oil  gas            jobs         oat           rice      tin            \n",
      "coffee       gnp            l-cattle     oilseed       rubber    trade          \n",
      "copper       gold           lead         orange        rye       veg-oil        \n",
      "copra-cake   grain          lei          palladium     ship      wheat          \n",
      "corn         groundnut      lin-oil      palm-oil      silver    wpi            \n",
      "cotton       groundnut-oil  livestock    palmkernel    sorghum   yen            \n",
      "cotton-oil   heat           lumber       pet-chem      soy-meal  zinc           \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import cmd # for printing a list as a column rather than as a row\n",
    "\n",
    "# Print the categories from the Reuters-21578 data set\n",
    "# We use the 'cmd' package to print the list based on a fixed width, otherwise the list\n",
    "# of categories would print on one single very long row, which is hard to read\n",
    "cli = cmd.Cmd()\n",
    "print(\"Reuters-21578 categories:\")\n",
    "print(cli.columnize(categories, displaywidth = 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Document tokenizing\n",
    " In this section, we define a tokenize() helper function to lowercase and tokenize the text, then stem each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words:\n",
      "a        but       hadn't   in        needn      she         they'll  when      \n",
      "about    by        has      into      needn't    she'd       they're  where     \n",
      "above    can       hasn     is        no         she'll      they've  which     \n",
      "after    couldn    hasn't   isn       nor        she's       this     while     \n",
      "again    couldn't  have     isn't     not        should      those    who       \n",
      "against  d         haven    it        now        shouldn     through  whom      \n",
      "ain      did       haven't  it'd      o          shouldn't   to       why       \n",
      "all      didn      having   it'll     of         should've   too      will      \n",
      "am       didn't    he       it's      off        so          under    with      \n",
      "an       do        he'd     its       on         some        until    won       \n",
      "and      does      he'll    itself    once       such        up       won't     \n",
      "any      doesn     her      i've      only       t           ve       wouldn    \n",
      "are      doesn't   here     just      or         than        very     wouldn't  \n",
      "aren     doing     hers     ll        other      that        was      y         \n",
      "aren't   don       herself  m         our        that'll     wasn     you       \n",
      "as       don't     he's     ma        ours       the         wasn't   you'd     \n",
      "at       down      him      me        ourselves  their       we       you'll    \n",
      "be       during    himself  mightn    out        theirs      we'd     your      \n",
      "because  each      his      mightn't  over       them        we'll    you're    \n",
      "been     few       how      more      own        themselves  we're    yours     \n",
      "before   for       i        most      re         then        were     yourself  \n",
      "being    from      i'd      mustn     s          there       weren    yourselves\n",
      "below    further   if       mustn't   same       these       weren't  you've    \n",
      "between  had       i'll     my        shan       they        we've  \n",
      "both     hadn      i'm      myself    shan't     they'd      what   \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get the stopwords corresponding to the English language\n",
    "stopwordsList = stopwords.words(\"english\")\n",
    "stopWordsEN = set(stopwordsList)\n",
    "\n",
    "# Print the stop words in multiple columns, where the total width of all the columns is 80\n",
    "print(\"Stop words:\")\n",
    "print(cli.columnize(stopwordsList, displaywidth = 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Helper function that tokenizes, lemmatizes and removes stopwords from a single document.\n",
    "#    Then the function returns also a document, but in processed and tokenized format.\n",
    "#    The main tokenization is made with the NLTK 'word_tokenize()' function.\n",
    "# Instead of stemming, we lemmatize, which is similar to stemming but offers better results\n",
    "# at the expense of higher CPU demand.\n",
    "#    The purpose of this function is to generate a cleaner, tokenized document for the TF-IDF\n",
    "# algorithm, which, in the end, requires a list of documents.\n",
    "#    This function can produce a single document and will be used a loop.\n",
    "def tokenizeDocument(document):\n",
    "    # First, tokenize the document passed as argument. We use NLTK 'word_tokenize()' which\n",
    "    # return a list of strings\n",
    "    tokens = nltk.word_tokenize(document, language = 'english')\n",
    "\n",
    "    # Filter tokens:\n",
    "    # - only alphabetical words are allowed\n",
    "    # - only words wich are not stopwords are allowed\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processedTokens = []\n",
    "    processedToken = \"\"\n",
    "    for token in tokens:\n",
    "        if (token.isalpha() and token not in stopWordsEN):\n",
    "            processedToken = lemmatizer.lemmatize(token.lower())\n",
    "            processedTokens.append(processedToken)\n",
    "\n",
    "    # Join the processed tokens into a single string, this string will represent\n",
    "    # a single document.\n",
    "    #    If we just join the tokens directly, they will just create a single very large word,\n",
    "    # but we are joining them in such a way that they have a space between them, by using\n",
    "    # \" \".join(list) - e.g. join the strings from a list and put a space between them.\n",
    "    #    In this way, the single string obtained will be recognized by the TF-IDF algorithm\n",
    "    # as an document of words, not a single large word (spaces are the standard delimiters that\n",
    "    # the TfidfVectorizer uses).\n",
    "    finalDocument = \" \".join(processedTokens) # note the space in the string (it's not an empty string)\n",
    "\n",
    "    return finalDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vectorize the Reuters-21578 data set by using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 531388 stored elements and shape (10788, 14157)>\n",
      "  Coords\tValues\n",
      "  (0, 840)\t0.08383479033397855\n",
      "  (0, 4564)\t0.09810947267937536\n",
      "  (0, 4713)\t0.07358504462157547\n",
      "  (0, 3129)\t0.07096776365365834\n",
      "  (0, 5144)\t0.034106087336237795\n",
      "  (0, 10838)\t0.05852669195279093\n",
      "  (0, 8232)\t0.04702388083660899\n",
      "  (0, 12909)\t0.32190522480092104\n",
      "  (0, 5126)\t0.04549954376580683\n",
      "  (0, 579)\t0.10632447902300628\n",
      "  (0, 6708)\t0.2996368734952329\n",
      "  (0, 10106)\t0.030222980594274796\n",
      "  (0, 538)\t0.0619506956598169\n",
      "  (0, 7643)\t0.030289969713393424\n",
      "  (0, 839)\t0.04153421639970694\n",
      "  (0, 4565)\t0.04050281377317544\n",
      "  (0, 8355)\t0.028614909948297303\n",
      "  (0, 10970)\t0.04060974363583497\n",
      "  (0, 2895)\t0.022506364378516868\n",
      "  (0, 3991)\t0.05166717162857884\n",
      "  (0, 1748)\t0.17225580121737882\n",
      "  (0, 8734)\t0.08674770081875548\n",
      "  (0, 11062)\t0.1540175036197175\n",
      "  (0, 12708)\t0.05461908075671201\n",
      "  (0, 12827)\t0.021140060382043697\n",
      "  :\t:\n",
      "  (10786, 8791)\t0.16546822694547178\n",
      "  (10786, 8446)\t0.13750480088068617\n",
      "  (10786, 11585)\t0.102878429311204\n",
      "  (10786, 11487)\t0.1497143456010575\n",
      "  (10786, 11346)\t0.11817729748339838\n",
      "  (10786, 6253)\t0.06696806822610876\n",
      "  (10786, 10010)\t0.07889426789982819\n",
      "  (10786, 3044)\t0.2668703854938214\n",
      "  (10786, 10767)\t0.17791640228947786\n",
      "  (10786, 1012)\t0.22384391921210278\n",
      "  (10786, 11494)\t0.2234049558810782\n",
      "  (10786, 8362)\t0.21203400942126208\n",
      "  (10786, 2026)\t0.22895171292402758\n",
      "  (10786, 8387)\t0.2724313015957913\n",
      "  (10787, 8083)\t0.2202508101472191\n",
      "  (10787, 7481)\t0.0963500466284545\n",
      "  (10787, 14085)\t0.12734486339297804\n",
      "  (10787, 12570)\t0.3102045817607599\n",
      "  (10787, 2866)\t0.1497840133721416\n",
      "  (10787, 8446)\t0.2836554690761472\n",
      "  (10787, 11487)\t0.30884225610235627\n",
      "  (10787, 3543)\t0.340026957525546\n",
      "  (10787, 3044)\t0.5505207374211383\n",
      "  (10787, 10767)\t0.18350981283756362\n",
      "  (10787, 990)\t0.4232211854700859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = []\n",
    "for fileId in fileIds:\n",
    "    documents.append(reuters.raw(fileId))\n",
    "\n",
    "processedDocuments = []\n",
    "for doc in documents:\n",
    "    processedDocuments.append(tokenizeDocument(doc))\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df = 0.85, min_df = 2)\n",
    "\n",
    "# Fit and then transform\n",
    "tfIdfMatrix = vectorizer.fit_transform(processedDocuments)\n",
    "\n",
    "print(tfIdfMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to do fitting and transforming over the processed document:\n",
    "    <blockquote>Fitting - will tell the TfidfVectorizer to learn the vocabulary and the importance of the words </blockquote>\n",
    "    <blockquote>Transform - will convert the document into numerical vectors with the information it learned </blockquote>\n",
    "\n",
    "Both of these are important so we can get a TfIDF matrix that we can use more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 8630\n",
      "Test data size: 2158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the date into training and testing sets\n",
    "# We're first creating labels (category) for each document \n",
    "labels = [reuters.categories(fileId)[0] for fileId in fileIds]\n",
    "\n",
    "# Doing the actual splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfIdfMatrix, labels, test_size=0.2, random_state=42)\n",
    "# The training set is going to be used for teaching the model patterns in the data\n",
    "# And the test set is used to ensure the model performs well on unseen data (so to ensure its\n",
    "# generalized well)\n",
    "# In our case we decided to keep just 20% of the data for testing \n",
    "# and i sued random state to be able to have the same split every time \n",
    "\n",
    "print(f\"Training data size: {X_train.shape[0]}\")\n",
    "print(f\"Test data size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Comparing with different mentrics and k-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: euclidean, k: 3, Accuracy: 0.6409, Time: 0.7418 seconds\n",
      "Metric: euclidean, k: 5, Accuracy: 0.5950, Time: 0.7020 seconds\n",
      "Metric: euclidean, k: 7, Accuracy: 0.5635, Time: 0.6842 seconds\n",
      "Metric: cosine, k: 3, Accuracy: 0.8230, Time: 0.6795 seconds\n",
      "Metric: cosine, k: 5, Accuracy: 0.8369, Time: 0.6812 seconds\n",
      "Metric: cosine, k: 7, Accuracy: 0.8448, Time: 0.6873 seconds\n",
      "Metric: manhattan, k: 3, Accuracy: 0.4272, Time: 1.2846 seconds\n",
      "Metric: manhattan, k: 5, Accuracy: 0.4106, Time: 1.1764 seconds\n",
      "Metric: manhattan, k: 7, Accuracy: 0.4082, Time: 1.0433 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "\n",
    "# Training KNN with different metrics using for in for\n",
    "# We will be using the following:\n",
    "# euclidian distance - which is the default for this \n",
    "# cosine distance - which can be informative when working with text\n",
    "# manhattan distance - an alternative to euclidean ?\n",
    "\n",
    "metrics =['euclidean', 'cosine', 'manhattan']\n",
    "k_values = [3, 5, 7]\n",
    "\n",
    "results = []\n",
    "\n",
    "for metric in metrics:\n",
    "    for k in k_values:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # We're initializing the KNN Classifier with the current metric and k value\n",
    "        knn = KNeighborsClassifier(n_neighbors=k,metric=metric,weights='distance')\n",
    "\n",
    "        # We're fitting the model \n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        # We're evaluating the model and tracking the results\n",
    "        accuracy = knn.score(X_test,y_test)\n",
    "        results.append((metric, k, accuracy))\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"Metric: {metric}, k: {k}, Accuracy: {accuracy:.4f}, Time: {end_time-start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go into an explanation about how the choice of distance metric and the number of neighbors affects the performance of KNN in terms of accuracy and time.\n",
    "\n",
    "<h3>1. Accuracy comparison</h3>\n",
    "We can notice how <b>Cosine metric</b> gives us the best accuracy across all of the k values compared to the other two choices.\n",
    "The other two can be considered significantly lower, with <b>Euclidean</b> performing a little bit better than <b>Manhattan</b> still.\n",
    "\n",
    "<h3>1.1 The reason why Cosine is better</h3>\n",
    "Cosine is used for text and high-dimension data, because, as we know, it measures the similarity between two documents by calculating the cosine of the angle between the vectors.\n",
    "As cosine performs the best for us, we can assume we have high-dimensional features and cosine better distincts between classes compared to the other ones.\n",
    "\n",
    "<h3>1.2 The reason Manhattan performs poorly</h3>\n",
    "Manhattan distance measures the sum of absolute differences of coordinates. It is usually used in grid-like structures, like city maps or certain robot motion planning scenarios. So in high-dimensional places it tends to fail to capture the actual closeness of points (and it leads to poor classification)\n",
    "\n",
    "<h3>2. Effect of k on Accuracy</h3>\n",
    "We notice that in Euclidean and Manhattan increasing the value of k will reduce accuracy, unlike cosine that increases with a higher k value, peaking at almost 85% with k=7.\n",
    "\n",
    "<h3>3. Time comparison</h3>\n",
    "Cosine and Euclidean are faster. Manhattan is slower (taking nearly double the time) because calculating absolute differences takes a lot longer compared to squared differences (Euclidean) or dot products (Cosine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Comparring with raw frequency (BOW?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Word Frequency - Metric: euclidean, k: 3, Accuracy: 0.7539, Time: 0.8411 seconds\n",
      "Raw Word Frequency - Metric: euclidean, k: 5, Accuracy: 0.7349, Time: 0.8233 seconds\n",
      "Raw Word Frequency - Metric: euclidean, k: 7, Accuracy: 0.7210, Time: 0.8062 seconds\n",
      "Raw Word Frequency - Metric: cosine, k: 3, Accuracy: 0.8471, Time: 0.8109 seconds\n",
      "Raw Word Frequency - Metric: cosine, k: 5, Accuracy: 0.8452, Time: 0.8099 seconds\n",
      "Raw Word Frequency - Metric: cosine, k: 7, Accuracy: 0.8494, Time: 0.8087 seconds\n",
      "Raw Word Frequency - Metric: manhattan, k: 3, Accuracy: 0.6186, Time: 2.8101 seconds\n",
      "Raw Word Frequency - Metric: manhattan, k: 5, Accuracy: 0.5931, Time: 2.8375 seconds\n",
      "Raw Word Frequency - Metric: manhattan, k: 7, Accuracy: 0.5737, Time: 3.1083 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# We're using count vectorizer to get the raw frequency for the words\n",
    "count_vectorizer = CountVectorizer(max_df=0.85, min_df=2)\n",
    "\n",
    "# Fitting and transforming raw word frequency\n",
    "frequency_matrix = count_vectorizer.fit_transform(processedDocuments)\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(frequency_matrix, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "for metric in metrics:\n",
    "    for k in k_values:\n",
    "        start_time = time.time()\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance')\n",
    "        knn.fit(X_train,y_train)\n",
    "        accuracy = knn.score(X_test,y_test)\n",
    "        end_time = time.time()\n",
    "        print(f\"Raw Word Frequency - Metric: {metric}, k: {k}, Accuracy: {accuracy:.4f}, Time: {end_time-start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go into the explanations for using raw word frequency extracted with CountVectorizer\n",
    "\n",
    "<h3>1. Accuracy comparison</h3>\n",
    "We can once again notice how Cosine outperforms the other two options we had (Euclidean and Manhattan). Now with raw word frequency we can say that Euclidean performs more decently, but Manhattan is still falling behind \n",
    "\n",
    "<h3>2. Effect of k on Accuracy</h3>\n",
    "It performs similar to above. we can see Euclidean and Manhattan decreasing in accuracy with the increase of k value, whereas Cosine remains quite stable.\n",
    "\n",
    "<h3>3. Time comparison</h3>\n",
    "It's noticeable, just like before, how Euclidean and Cosine run a lot faster, again compared to Manhattan that is very slow.\n",
    "\n",
    "Again, we arrive at the conclusion that Cosine is our best option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Apply the PCA and t-SNE algorithms for reducing the dimensionality of the data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of PCA-reduced matrix: (10788, 2)\n",
      "Explained variance ratio of PCA: 0.07506011215752095\n",
      "Shape of TSNE-reduced matrix: (10788, 2)\n"
     ]
    }
   ],
   "source": [
    "# This code will take about 1min 30sec to finish.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Convert sparse matrix to dense matrix\n",
    "denseMatrix = tfIdfMatrix.toarray()\n",
    "\n",
    "# Reduce the dimensionality of the data set by using the PCA algorithm. We pass as main argument\n",
    "# the number of principal components: this is bassicaly the new dimension of the\n",
    "# converted data. The lower the dimension, the simpler the data becomes, but we also risk\n",
    "# loosing important information from the data.\n",
    "pca = PCA(n_components = 2)\n",
    "reducedPCAMatrix = pca.fit_transform(denseMatrix)\n",
    "print(\"Shape of PCA-reduced matrix:\", reducedPCAMatrix.shape)\n",
    "print(\"Explained variance ratio of PCA:\", np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Reduce the dimensionality of the data set by using the t-SNE algorithm.\n",
    "#    We use a very small number of principal components because the t-SNE algorithm uses by default\n",
    "# the 'barnes_hut' algorithm for calculating the gradient and that requires a number of principal\n",
    "# components smaller than 4.\n",
    "#    If we use the 'exact' algorithm for gradient calculation, the performance is going to be poor. \n",
    "tsne = TSNE(n_components = 2, random_state = 0)\n",
    "reducedTSNEMatrix = tsne.fit_transform(denseMatrix)\n",
    "print(\"Shape of TSNE-reduced matrix:\", reducedTSNEMatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.17973043  0.0836326 ]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[-64.87492   35.620476]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.13454316  0.03090986]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[ 56.724167 -41.84875 ]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.14066232  0.03542713]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[ 27.507524 -62.834003]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.178002    0.08741341]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[  4.270859 -28.907513]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.1588988   0.04589015]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[-30.512508 -15.358434]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.15298924  0.00391352]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[-23.92899   15.120278]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.16472713  0.02986478]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[65.772194 32.331203]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.14906313  0.00584545]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[-27.602755   -9.0935955]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.1293672   0.02227895]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[-70.24326   56.059116]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.16168847  0.02786804]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[ 55.734104 -62.962067]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.14250841  0.00827923]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[44.40465 76.44588]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.15879932  0.04923771]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[ -6.969405 -28.621277]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.08233518  0.06773545]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[ -55.12807 -105.01757]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.1594526  0.0509965]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[ 50.851543 -75.30099 ]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.18662119  0.04306989]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[-34.63492   13.289241]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.11263374 -0.02532265]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[-11.924319     0.99301517]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[0.1760672  0.08895807]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[23.802967 34.437798]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.065465    0.06304062]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[17.385561 57.797516]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.15596381  0.09056154]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[-17.50604   24.821547]\n",
      "\n",
      "PCA-reduced Vector for Example Document:\n",
      "[-0.13712086  0.02631123]\n",
      "\n",
      "t-SNE-reduced Vector for Example Document:\n",
      "[96.48254  71.237495]\n"
     ]
    }
   ],
   "source": [
    "# Print the first few PCA-reduced documents and t-SNE-reduced documents\n",
    "for docIndex in range(20):\n",
    "    # Print the current PCA-reduced document\n",
    "    print(\"\\nPCA-reduced Vector for Example Document:\")\n",
    "    print(reducedPCAMatrix[docIndex])\n",
    "\n",
    "    # Print the current t-SNE-reduced document\n",
    "    print(\"\\nt-SNE-reduced Vector for Example Document:\")\n",
    "    print(reducedTSNEMatrix[docIndex])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
